{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from multiprocessing import Process\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle \n",
    "import bz2\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import regex as re\n",
    "import cld3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from clickhouse_driver import connect\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import math\n",
    "from scipy.sparse import csr_matrix, vstack, save_npz, load_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Merkmalsextraktion von Tweets </h1>\n",
    "\n",
    "Dieses Notebook behandelt die Extraktion von Tweetmerkmalen. **Vor Ausführung dieses Skriptes muss bereits ein kMeans-Clustering durchgeführt und persistiert worden sein. Dieses wurde im Notebook 1-CreateLabels.ipynb implementiert.**\n",
    "\n",
    "Die meisten extrahierbaren Merkmale lassen sich direkt aus den Tweets entnehmen und ohne komplexere Vorverarbeitung in eine numerische Repräsentation umwandeln. Bei anderen Merkmalen ist dies nicht ohne weitere Schritte möglich. Konkret brauchen die Textpassagen des Algorithmus eine zusätzliche Verarbeitung durch ein vortrainiertes Word Embedding Tool und die Ortsangaben aus den Nutzerprofilen eine Konvertierung in Koordinaten durch einen Geocoder.\n",
    "\n",
    "Die Extraktion wird daher in mehreren getrennten Schritten umgesetzt. Folgende Schritte sind dabei exemplarisch vorimplementiert:\n",
    "\n",
    "1. Die Konvertierung von Tweets in eine CSV-Repräsentation, welche eine unvollständige und noch nicht vollends numerische Repräsentation aller Merkmale hält.\n",
    "2. Eine exemplarische Implementierung des Profil-Location Geocoders.\n",
    "3. Ein Modell zur Bestimmung von lokalen Wörtern aus Texten.\n",
    "4. Die fastText Skripte zum Training und Labeling von Texten (auf Bash).\n",
    "\n",
    "\n",
    "<h2>1. Konvertierung von Tweets in eine CSV-Repräsentation relevanter Merkmale</h2>\n",
    "\n",
    "Die Konvertierung von Tweets ist hier in einer parallel implementierten Umgebung umgesetzt. Es wird eine feste Zahl an Prozessen erzeugt, welchen jeweils ein zugewiesenes Tweet-File umwandeln. Aus einem Geotweet File (vgl. Vorverarbeitung in Bash) werden dann mehrere temporäre Files erzeugt, welche zu jedem Tweet Teilinformationen beinhalten, die dann später getrennt voneinander weiterverarbeitet werden. \n",
    "\n",
    "Wie ein Tweet verarbeitet wird, ist in der nachfolgend ersichtlichen *run*-Funktion implementiert. Die Generierung von Arbeiterprozessen und die dynamische Zuordnung von Arbeitspaketen kann dann weiter unten in der nächsten Zelle eingesehen werden.\n",
    "\n",
    "Zu Beginn der *run()*-Funktion sind einige Hilfsmethoden definiert, welche verschiedene später aus dem Tweet extrahierte Merkmale in einen festen Wertebereich $[-1; 1]$ normieren. Beispielsweise wird die UTC-referenzierte Uhrzeit, wie im Bericht beschrieben, so konvertiert, dass ein Wert von $-1$ auf Mitternacht, $0$ auf Mittags und $1$ auf 24 Uhr des nächsten Tages referenziert. Alle anderen Tageszeiten werden linear auf einen entsprechenden Wert dazwischen interpoliert.\n",
    "\n",
    "Die weiter unten stehende Hilfsfunktion *tweet_to_dict()* führt die eigentliche Konvertierung von **exakt einem** Tweetobjekt durch. Konkret nimmt die Funktion die JSON-Repräsentation eines Tweets entgegen, d.h. ein Dictionary, und gibt schließlich wieder ein Dictionary aus. Das Ausgabedict ist eine kompaktere Form eines Tweets, welches als Schlüssel nur die, für die Vorhersage relevanten, Merkmale enthält. Die zugehörigen Wertepaare dieser Schlüssel werden jedoch - je nach benötigter Vorverarbeitung - nur teilweise in die schlussendliche numerische Form konvertiert. Für zeitliche oder popularitätsbezogene Merkmale wird dies beispielsweise direkt durchgeführt. Profil-Location von Nutzern oder Tweet-Texte werden zwar extrahiert und persistiert, müssen jedoch nachträglich von anderen Komponenten weiterverarbeitet werden.\n",
    "\n",
    "Unter soeben beschriebener Hilfsmethode (d.h. ab dem Kommentarblock namens \"*continuation of work flow*\") findet nun die eigentliche Verarbeitung von zugewiesenen Tweet Files statt. Wie bereits beschrieben wird ein Tweet in mehrere temporäre Ergebnisfiles persistiert, die attributsbezogen gruppiert sind. So entstehen aus einem Tweet-File im *geotagged/*-Ordner drei neue Files:\n",
    "- ein gleichnamiges **.texts*-File, welches nur eine vorverarbeitete Version des Tweettextes (zur Vorhersage eines Text-Modells) enthält,\n",
    "- ein gleichnamiges **.geocode*-File, welches nur eine vorverarbeitete Version der Profil-Location (zur Vorhersage eines Geocoders) enthält und\n",
    "- gleichnamiges **.features*-File, welches alle verbleibenden Merkmale als CSV-Datei ablegt. Numerische Merkmale werden hierin direkt in die entsprechende Feature-Repräsentation überführt. Kategorische Merkmale, bspw. Sprache oder Quellplattform verbleiben im CSV-File zunächst in ihrer String-Repräsentation und werden im nachfolgenden Machine Learning Skript durch entsprechende Encoder in eine numerische Repräsentation überführt. \n",
    "\n",
    "Wie im Code ersichtlich, wird jedem der drei Files zunächst eine Header-Line hinzugefügt, sodass die nachträgliche Zuordnung von Werten zu Merkmalen problemlos erfolgen kann. Danach wird der als Parameter übergebene File-Path verarbeitet. Dieser verweist auf das Tweet-File, welches der aktuelle Prozess zu verarbeiten hat. Dieses wird zeilenweise (also Tweet-weise) ausgelesen und der entsprechende Tweet der oben beschriebenen *tweet_to_dict()*-Hilfsfunktion übergeben. Das zurückgelieferte und aufbereitete Tweetfile wird dann Komponentenweise in die drei Ausgabedateien persistiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(file_path):   \n",
    "    # in einem parallelisierten Umfeld müssen geteilte Variablen explizit deklariert werden\n",
    "    global TSTAMP_OFFSET\n",
    "    global path_to_place_model\n",
    "    place_model = pickle.load(open(path_to_place_model, \"rb\"))\n",
    "    out_file = \"data/feature_files/\" + file_path[:-6].split(\"/\")[-1]\n",
    "    \n",
    "    \"\"\"\n",
    "        Define some normalization functions for converting tweets to feature vectors\n",
    "    \"\"\"\n",
    "    def to_hour(timestamp_ms):\n",
    "        return ((timestamp_ms / 1000) % 86400) / 3600\n",
    "\n",
    "    def norm_time(timestamp_ms):\n",
    "        return (to_hour(timestamp_ms) - 12) / 12  # -1 = 0 Uhr UTC, 0 = 12 Uhr UTC, 1 = 24 Uhr UTC\n",
    "    \n",
    "    def norm_ego(count, max_exp=5):\n",
    "        count = np.log10(count + 1)\n",
    "        count = count if count <= max_exp else max_exp  # max_exp gibt an, ab welchen werten auf 1 skaliert wird\n",
    "        return (count - max_exp/2) / (max_exp/2)\n",
    "    \n",
    "    def predict_language(text):\n",
    "        pred = cld3.get_language(text)\n",
    "        try:\n",
    "            if pred.is_reliable:\n",
    "                return pred.language\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    \"\"\"\n",
    "        This function extracts the features of a single tweet\n",
    "    \"\"\"\n",
    "    def tweet_to_dict(tweet):\n",
    "        d = {}\n",
    "        \n",
    "        # obtain label\n",
    "        place = tweet.get(\"place\")\n",
    "        \n",
    "        if place is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            place = place.get(\"bounding_box\").get(\"coordinates\")\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        place = np.asarray(place)[0].mean(axis=0).reshape(1, -1)\n",
    "        place_T = np.asarray([place[0][1], place[0][0]]).reshape(1, -1)\n",
    "        d[\"label\"] = place_model.predict(place_T)[0]\n",
    "        \n",
    "        # Text Preprocessing\n",
    "        text = re.sub(r'[\\s]+', ' ',  # remove multiple space characters\n",
    "          re.sub(r'\\b\\w{1,2}\\b', '',  # remove tokens with less than 3 characters\n",
    "          re.sub(r'[^\\p{Arabic}\\p{Greek}\\p{Cyrillic}\\p{Latin}\\p{Han}\\p{Hangul} ]', '',  # some dirty multilingual regex ;-)\n",
    "          re.sub(r'[\\n\\t]', ' ',  # remove linebreaks and tabs\n",
    "          re.sub(r'@\\w+', '',  # remove @ annotations\n",
    "          re.sub(r'https?:\\/\\/.*[\\r\\n]*', '',\n",
    "          tweet.get(\"text\"))))))).strip()  # remove urls\n",
    "        \n",
    "        d[\"text\"] = text\n",
    "        \n",
    "        # TID, TS, DC, WT\n",
    "        tid = int(tweet.get(\"id_str\"))\n",
    "        ts = (tid >> 22) + TSTAMP_OFFSET \n",
    "        dc = (tid >> 17) & 0b11111\n",
    "        wt = (tid >> 12) & 0b11111\n",
    "        d[\"ts\"] = norm_time(ts)\n",
    "        d[\"dc\"] = dc\n",
    "        d[\"wt\"] = wt\n",
    "                        \n",
    "        # Language\n",
    "        lang = tweet.get(\"lang\")\n",
    "        \n",
    "        # restrict on some languages for the prototype\n",
    "        languages = [\"en\", \"de\", \"ru\", \"fr\", \"es\", \"pt\", \"ar\", \"zh\", \"ko\"]  \n",
    "        \n",
    "        if lang not in languages:\n",
    "            return None\n",
    "        \n",
    "        d[\"lang\"] = lang\n",
    " \n",
    "        \n",
    "        # User Obj.\n",
    "        if tweet.get(\"user\") is None:\n",
    "            d['user_friends'] = None\n",
    "            d['user_followers'] = None\n",
    "            d['user_statuses'] = None\n",
    "            d['user_lists'] = None\n",
    "            d['user_account_created'] = None\n",
    "            d['user_pred_lang'] = None\n",
    "            d['user_place'] = None\n",
    "        else:\n",
    "            user_friends = tweet.get(\"user\", {}).get(\"friends_count\") \n",
    "            user_followers = tweet.get(\"user\", {}).get(\"followers_count\") \n",
    "            user_statuses = tweet.get(\"user\", {}).get(\"statuses_count\") \n",
    "            user_lists = tweet.get(\"user\", {}).get(\"listed_count\")\n",
    "            user_account_created = tweet.get(\"user\", {}).get(\"created_at\") \n",
    "            user_pred_lang = None\n",
    "            try:\n",
    "                user_pred_lang = tweet.get(\"user\", {}).get(\"description\", \"\").strip().replace(\"\\n\", \" \").replace(\";\", \" \")\n",
    "            except:\n",
    "                pass\n",
    "            user_place = None\n",
    "            try:\n",
    "                user_place = tweet.get(\"user\", {}).get(\"location\", \"\").strip().replace(\"\\n\", \" \").replace(\";\", \" \").replace(\",\", \" \")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if user_friends is None:\n",
    "                d['user_friends'] = None\n",
    "            else:\n",
    "                d['user_friends'] = norm_ego(user_friends)\n",
    "            if user_followers is None:\n",
    "                d['user_followers'] = None\n",
    "            else:\n",
    "                d['user_followers'] = norm_ego(user_followers)\n",
    "            if user_statuses is None:\n",
    "                d['user_statuses'] = None\n",
    "            else:\n",
    "                d['user_statuses'] = norm_ego(user_statuses)\n",
    "            if user_lists is None:\n",
    "                d['user_lists'] = None\n",
    "            else:\n",
    "                d['user_lists'] = norm_ego(user_lists)\n",
    "            if user_account_created is None:\n",
    "                d['user_account_created'] = None\n",
    "            else:\n",
    "                user_account_created = datetime.datetime.strptime(user_account_created, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "                d['user_account_created'] = norm_time(user_account_created.timestamp())\n",
    "            if user_pred_lang is None:\n",
    "                d['user_pred_lang'] = None\n",
    "            else:\n",
    "                user_pred_lang = predict_language(user_pred_lang)\n",
    "                d['user_pred_lang'] = user_pred_lang if user_pred_lang is not None else d[\"lang\"]\n",
    "            if user_place is None:\n",
    "                d['user_place'] = None\n",
    "            else:\n",
    "                d['user_place'] = user_place if len(user_place) > 1 else None\n",
    "\n",
    "        source = BeautifulSoup(tweet.get(\"source\")).text\n",
    "        d[\"source\"] = source\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    \"\"\"\n",
    "        Continuation of work flow\n",
    "    \"\"\"    \n",
    "    texts_file = open(out_file + \"texts\", \"w\")\n",
    "    geocode_file = open(out_file + \"geocode\", \"w\")\n",
    "    feature_file = open(out_file + \"features\", \"w\")\n",
    "    label_file = open(out_file + \"label\", \"w\")\n",
    "    \n",
    "    # Write Headers\n",
    "    texts_file.write(\"text\\n\")\n",
    "    geocode_file.write(\"profile_location\\n\")\n",
    "    feature_file.write(\"label;post_created;dc;wt;language;friends;followers;statuses;lists;account_created;profile_language;source\\n\")\n",
    "    label_file.write(\"label\\n\")\n",
    "    \n",
    "    debug_text = False\n",
    "    \n",
    "    with bz2.open(file_path, \"rt\") as file:\n",
    "        # for line in file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            tweet_dict = tweet_to_dict(tweet)\n",
    "                        \n",
    "            if tweet_dict is None:  # out of sample language\n",
    "                if debug_text:\n",
    "                    print(\"< Skipping tweet with out of sample language>\")\n",
    "                continue\n",
    "\n",
    "            if debug_text:  # if console output for debugging\n",
    "                for k, v in tweet_dict.items():\n",
    "                    print(\">>{}: {}\".format(k, v))\n",
    "                print(\"--------------------------\")\n",
    "            \n",
    "            # persist tweet\n",
    "            label_file.write(\"{}\\n\".format(tweet_dict[\"label\"]))\n",
    "            texts_file.write(\"{}\\n\".format(tweet_dict[\"text\"]))\n",
    "            geocode_file.write(\"{}\\n\".format(tweet_dict[\"user_place\"]))\n",
    "            del tweet_dict[\"text\"]\n",
    "            del tweet_dict[\"user_place\"]\n",
    "            feature_file.write(\"{}\\n\".format(\";\".join([str(v) for v in tweet_dict.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgende Zelle zeigt die Zuordnung von Tweetfiles zu individuellen Prozessen. Alle Geotweets sind hierbei zuvor in den Ordner *data/geotagged/* extrahiert worden (vgl. vorherige Skripte). Alle in diesem Ordner enthaltenene Dateipfade werden zunächst aufgelistet und in der *files* Variable niedergeschrieben. Für jeden Dateipfad wird ein eigener *worker*-Prozess erzeugt, welcher mit Hilfe der oben beschriebenen *run()*-Methode aus den im Dateipfad enthaltenen Tweets drei Ausgabedateien erzeugt.\n",
    "\n",
    "Nachdem die Prozesse gespawnt wurden, müssen sie mit *worker.start()* gestartet werden. Mit *worker.join()* kann im Hauptprozess ein Wartepunkt gesetzt werden, der die nachfolgende Programmausführt so lange verzögert, bis alle Arbeiterprozesse abgeschlossen sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All workers finished\n"
     ]
    }
   ],
   "source": [
    "TSTAMP_OFFSET = 1288834974657\n",
    "\n",
    "folder = \"data/geotagged/\"\n",
    "files = sorted([folder + f for f in listdir(folder) if isfile(join(folder, f)) and f[-6:] == \"result\"])\n",
    "path_to_place_model = \"kmeans_150.pcl\"\n",
    "\n",
    "\n",
    "workers = [Process(target=run, args=(file,)) for file in files]\n",
    "\n",
    "for w in workers:\n",
    "    w.start()\n",
    "    \n",
    "for w in workers:\n",
    "    w.join()\n",
    "    \n",
    "print(\"All workers finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Geocoding von Profil-Locations</h2>\n",
    "\n",
    "Geocoding bezieht sich auf das Verfahren, einen Ortstext auf eine konkrete Weltkoordinate abzubilden. Prinzipiell existieren dafür mehrere Dienste, welche wahlweise open source oder bezahlt sind. Bei Open Source besteht prinzipiell die Problematik, dass meist Anfragelimitierungen existieren. Im Projekt wurde das Geocoding über eine interne Datenbank umgesetzt, wie unten gezeigt. Gleichzeitig wird aber exemplarisch eine alternative Implementierung über das OSM-Tool Nominatim angedeutet.\n",
    "\n",
    "Wir beginnen zunächst mit der internen Lösung mit der Datenbank *clickhouse*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLatLong(synonym_list):\n",
    "    conn = connect('clickhouse://default:clickhouse2020@localhost/superset')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT latitude, longitude , synonym\\\n",
    "        FROM cities \\\n",
    "        WHERE synonym in %(synonym)s AND population >= 1000 \\\n",
    "        ORDER BY synonym, population DESC \\\n",
    "        LIMIT 1 BY synonym\", {'synonym': synonym_list})\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    records = cursor.fetchall()\n",
    "    iResults = len(records)\n",
    "    return {x[2]:x[:2] for x in records}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clickhouse erlaubt Batchanfragen, was bei der Verarbeitung größerer Mengen an Tweets von immensem Vorteil gegenüber einzelnen Anfragen ist. Wie unten im Code ersichtlich, ist die Batchgröße auf $200$ gesetzt. \n",
    "\n",
    "Die Profil-Locations von Nutzern wurden bereits im vorherigen Skript *1-CreateLabels.ipynb* extrahiert und aufbereitet. Die entsprechenden Location-Strings wurden daraufhin in entsprechenden data/feature_files/\\*.geocode-Dateien abgelegt. Diese Dateien werden nun schrittweise ausgelesen und der darin enthaltene Location-Text nach weiterer Vorverarbeitung schrittweise in die Anfragebatches eingefüllt. Wie die Vorverarbeitung aufgebaut ist, wird weiter unten beschrieben.\n",
    "\n",
    "Nach der Verarbeitung der Datenbank werden die Geocodes als Lat/Long-Paare zurückgeliefert. Die *profile_location*, wie sie auch vom Algorithmus verarbeitet wird, erwartet jedoch keine Koordinatenpaare sondern diskrete Labels als Featurewerte. Wir nutzen hierzu das kMeans-Clustering Modell, welches bereits für die Erstellung der Zielklassen verwendet wird. Mit Hilfe der *predict()* Funktion können somit die Koordinaten auf eines der $150$ Zielklassen abgebildet werden, falls die Geocoding Anfrage zu einem Ergebnis geführt hat. In jedem anderen Fall (bspw. dann, wenn der Profilort leer war), wird ein Defaultwert von $-1$ zugeordnet. Die Ergebnisse aller Tweets werden dann schrittweise in einem neu angelegten File **.loc* abgelegt.\n",
    "\n",
    "Hier noch einige Notizen zur Vorverarbeitung. Clickhouse (als verwendeter Geocoder) ist in der Lage, Synonyme von Ortsnamen entgegen zu nehmen. Bei der Disambiguierung mehrdeutiger Namen referenziert die Datenbank jedoch immer auf den Ort mit der größten Übereinstimmung oder bei mehreren Orten auf denjenigen mit den meisten Einwohnern. Umgekehrt ist die Datenbank (anders als viele andere Geocoder) jedoch nicht immer in der Lage, Landeskürzel oder variierende Schreibweisen verlässlich vorherzusagen, insbesondere dann, wenn der Ort aus mehreren Wörtern besteht. Um den Output des Geocoders zu verbessern, wurde ein provisorisches (jedoch nicht ausgereiftes) Preprocessing durchgeführt. Hierzu wurde in unten stehendem Codefragment schrittweise versucht, aus dem Ortstext zunächst einen Ort mit drei, dann mit zwei und zuletzt mit einem Wort zu extrahieren und vorherzusagen. Dediziertere Geocoder kommen auch ohne eine derartige Form der Vorverarbeitung aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:50<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "\n",
    "folder = \"data/feature_files/\"\n",
    "files = sorted([folder + f for f in listdir(folder) if isfile(join(folder, f)) and f[-7:] == \"geocode\"])\n",
    "path_to_place_model = \"kmeans_150.pcl\"\n",
    "\n",
    "place_model = pickle.load(open(path_to_place_model, \"rb\"))\n",
    "\n",
    "# Keep the model sequential to not overload our database\n",
    "for file in tqdm(files):\n",
    "    out_file = open(folder + file.split(\"/\")[-1][:-7] + \"loc\", \"w\")\n",
    "    out_file.write(\"profile_location\\n\")\n",
    "    \n",
    "    # geocode one file\n",
    "    with open(file, \"r\") as f:\n",
    "        next(f)  # skip header\n",
    "        \n",
    "        # iterate as long as new batches are available\n",
    "        data_available = True\n",
    "        while data_available:\n",
    "\n",
    "            # get next batch\n",
    "            batch_raw = []\n",
    "            try:\n",
    "                # try to access next line until either batch_size or EOF is reached\n",
    "                for _ in range(batch_size):\n",
    "                    # some text preprocessing\n",
    "                    text = next(f).strip()\n",
    "                    text = re.sub(r'[\\s]+', ' ',  # remove multiple space characters\n",
    "                      re.sub(r'[^\\p{Arabic}\\p{Greek}\\p{Cyrillic}\\p{Latin}\\p{Han}\\p{Hangul} ]', '', text))\n",
    "                    # add to batch\n",
    "                    batch_raw.append(text)\n",
    "            except StopIteration:\n",
    "                # EOF reached\n",
    "                data_available = False\n",
    "                \n",
    "            result = {}\n",
    "\n",
    "            \"\"\"\n",
    "                First Iteration: resolve location descriptions with three words\n",
    "            \"\"\"\n",
    "            # initialize some variables for request and mapping\n",
    "            batch = []\n",
    "            indices = []\n",
    "\n",
    "            # Iterate over all location descriptions\n",
    "            for i, item in enumerate(batch_raw): \n",
    "                splits = item.split(\" \")\n",
    "                # and filter on those with three or more words\n",
    "                if len(splits) > 2:\n",
    "                    # utilize the first three words while capitalizing the first and third one\n",
    "                    # example: Rio de Janeiro\n",
    "                    batch.append(\" \".join([splits[0].capitalize(), splits[1], splits[2].capitalize()]))\n",
    "                    # add index of location description for mapping the position within the batch\n",
    "                    indices.append(i)\n",
    "\n",
    "            # result is a dictionary of the form {\"requested_item_name\": (lat, long), ...}\n",
    "            res = getLatLong(batch)\n",
    "            # map from item name back to its position within the batch\n",
    "            for k, v in res.items():\n",
    "                result[indices[batch.index(k)]] = v\n",
    "\n",
    "            \"\"\"\n",
    "                Second Iteration: resolve location descriptions with two words\n",
    "            \"\"\"\n",
    "\n",
    "            # reset variables\n",
    "            batch = []\n",
    "            indices = []\n",
    "\n",
    "            # iterate over all location descriptions\n",
    "            for i, item in enumerate(batch_raw):\n",
    "                splits = batch_raw[i].split(\" \")\n",
    "                # and filter on those with two or more words (that also yielded no results for three words)\n",
    "                if len(splits) > 1 and i not in result:\n",
    "                    # utilize the first two words and capitalizing both\n",
    "                    # example: Sao Paolo, New York\n",
    "                    batch.append(\" \".join(splits[:2]).title())\n",
    "                    # map from item name back to its position within the batch\n",
    "                    indices.append(i)\n",
    "\n",
    "            res = getLatLong(batch)\n",
    "            for k, v in res.items():\n",
    "                result[indices[batch.index(k)]] = v\n",
    "\n",
    "            \"\"\"\n",
    "                Third Iteration: resolve location descriptions with one word\n",
    "            \"\"\"\n",
    "            # reset variables\n",
    "            batch = []\n",
    "            indices = []\n",
    "\n",
    "            # Iterate over all location descriptions\n",
    "            for i, item in enumerate(batch_raw): \n",
    "                if i not in result:\n",
    "                    batch.append(item.split(\" \")[0].capitalize())\n",
    "                    indices.append(i)\n",
    "\n",
    "            res = getLatLong(batch)\n",
    "\n",
    "            for k, v in res.items():\n",
    "                result[indices[batch.index(k)]] = v\n",
    "\n",
    "            \"\"\"\n",
    "                Convert (lat, long)-coordinates to labels\n",
    "            \"\"\"\n",
    "            for k, v in result.items():\n",
    "                result[k] = place_model.predict(np.asarray(v).reshape(1, -1))[0]\n",
    "\n",
    "            \"\"\"\n",
    "                Persist results\n",
    "            \"\"\"\n",
    "\n",
    "            # write one line per item in the batch\n",
    "            for i in range(len(batch_raw)):\n",
    "                # write predicted label if present\n",
    "                if i in result:\n",
    "                    out_file.write(\"{}\\n\".format(result[i]))\n",
    "                else:\n",
    "                # else write -1\n",
    "                    out_file.write(\"-1\\n\")\n",
    "\n",
    "            # force the file to write lines\n",
    "            out_file.flush()\n",
    "             \n",
    "        # end while data available\n",
    "    # end with open\n",
    "# end for file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anbei ist ein Codebeispiel mit Nominatim zu sehen. Zunächst muss unter der Angabe eines aussagekräftigen *user_agent* Namens ein Nominatim-Client erstellt werden. Mit der Schnittstelle *client*.geocode(*str*) kann ein einzelner übergebener String geparsed werden. Existiert eine Rückgabe $\\neq$ *None*, so kann durch die beiden Attribute *tweet_location.latitude* und *tweet_location.longitude* die Weltkoordinate entnommen werden. Diese wird, äquivalent zu obigem Codefragment, mit dem kMeans-Clusteringmodell in eine Polygon-ID vorhergesagt, welche dann alternativ in das **.loc*-File als Ausgabe geschrieben werden könnte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 'Meath' is translated to coordinates (53.649784350000004, -6.588529492009938). Cluster Prediction: [72]\n",
      "Location 'Mossoró Brasil' is translated to coordinates (-5.1904332, -37.3443872). Cluster Prediction: [25]\n",
      "Location 'London' is translated to coordinates (51.5073219, -0.1276474). Cluster Prediction: [85]\n",
      "Location 'Kuala Terengganu Terengganu' is translated to coordinates (5.3296461, 103.1383265). Cluster Prediction: [59]\n",
      "Location 'Jersey City NJ' is translated to coordinates (40.7215682, -74.047455). Cluster Prediction: [29]\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"Tweetlocator_workshop\")\n",
    "\n",
    "folder = \"data/feature_files/\"\n",
    "files = sorted([folder + f for f in listdir(folder) if isfile(join(folder, f)) and f[-7:] == \"geocode\"])\n",
    "\n",
    "path_to_place_model = \"kmeans_150.pcl\"\n",
    "kmeans = pickle.load(open(path_to_place_model, \"rb\"))\n",
    "max_count = 5\n",
    "\n",
    "\n",
    "# geocode one file\n",
    "with open(files[0], \"r\") as f:\n",
    "    next(f)  # skip header\n",
    "\n",
    "    # iterate as long as new batches are available\n",
    "    data_available = True\n",
    "    while data_available:\n",
    "        try:\n",
    "            # try to access next line until either batch_size or EOF is reached\n",
    "            text = next(f).strip()\n",
    "            text = re.sub(r'[\\s]+', ' ',  # remove multiple space characters\n",
    "              re.sub(r'[^\\p{Arabic}\\p{Greek}\\p{Cyrillic}\\p{Latin}\\p{Han}\\p{Hangul} ]', '', text))\n",
    "            if str(text) != \"None\":\n",
    "                tweet_location = geolocator.geocode(text)\n",
    "                predicted_cluster = kmeans.predict(np.asarray((tweet_location.latitude, tweet_location.longitude)).reshape(1, -1))\n",
    "\n",
    "                print(\"Location '{}' is translated to coordinates ({}, {}). Cluster Prediction: {}\".format(text, \n",
    "                                                                                   tweet_location.latitude, \n",
    "                                                                                   tweet_location.longitude,\n",
    "                                                                                    predicted_cluster))\n",
    "                max_count -= 1\n",
    "\n",
    "            if max_count == 0:\n",
    "                data_available = False\n",
    "        except StopIteration:\n",
    "            # EOF reached\n",
    "            data_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Extraktion von lokalen Worten</h2>\n",
    "\n",
    "Für die optionale Extraktion bzw. das Filtering von lokalen Worten kann beispielsweise ein Inverse Location Frequency (ILF) Score pro Wort im Vokabular berechnet werden. In obigem Arbeitsablauf ist diese Komponente nicht implementiert, nachfolgend soll aber gezeigt werden, wie eine derartige Filterliste erstellt werden kann. Diese könnte dann im obigen Ablauf eingebaut werden.\n",
    "\n",
    "Zur Erstellung der Filterliste wird wieder ein Trainingskorpus mit gelabelten Texten benötigt. Im unten stehenden Code verwenden wir sehr wenige Tweetfiles (für ein sauberes Anwendungsszenario werden natürlich mehr Daten benötigt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 54.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.745808764210109, 'pdx\\n')\n",
      "(10.745808764210109, 'Rom')\n",
      "(10.745808764210109, 'PORT')\n",
      "(10.745808764210109, 'IMFC\\n')\n",
      "(10.745808764210109, 'Buenos')\n",
      "(10.052661583650163, 'بالخاطركلام')\n",
      "(10.052661583650163, 'HOPE')\n",
      "(10.052661583650163, 'Acción')\n",
      "(9.647196475542, 'ايه')\n",
      "(9.647196475542, 'tragedy')\n",
      "(9.647196475542, 'Portland')\n",
      "(9.647196475542, 'Florida\\n')\n",
      "(9.647196475542, 'Avenue')\n",
      "(9.359514403090218, 'und')\n",
      "(9.359514403090218, 'satélites')\n",
      "(9.359514403090218, 'passando')\n",
      "(9.359514403090218, 'kkkkkkkkkk\\n')\n",
      "(9.359514403090218, 'Sem')\n",
      "(9.359514403090218, 'Janeiro\\n')\n",
      "(9.359514403090218, 'California\\n')\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "N_documents = 0\n",
    "min_th = 15\n",
    "\n",
    "folder = \"data/feature_files/\"\n",
    "files = sorted([folder + f for f in listdir(folder) if isfile(join(folder, f)) and f[-5:] == \"texts\"])\n",
    "\n",
    "for file in tqdm(files):\n",
    "    text_file = open(file, \"r\")\n",
    "    label_file = open(file.split(\"texts\")[0] + \"label\", \"r\")\n",
    "\n",
    "    next(text_file)\n",
    "    next(label_file)\n",
    "\n",
    "\n",
    "    # Sammle Informationen über Wörter des Textes\n",
    "    data_available = True\n",
    "    while data_available:\n",
    "        try:\n",
    "            text = next(text_file)\n",
    "            label = next(label_file)\n",
    "\n",
    "            words = text.split(\" \")\n",
    "            if label is not \"-1\":\n",
    "                for word in words:\n",
    "                    if word not in word_dict:\n",
    "                        word_dict[word] = np.zeros(150)\n",
    "                    word_dict[word][int(label)] += 1\n",
    "                N_documents += 1\n",
    "        except StopIteration:\n",
    "            # EOF\n",
    "            data_available = False\n",
    "        \n",
    "# Berechne ILF Score für Wörter des Textes\n",
    "ILF_scores = []\n",
    "for k, v in word_dict.items():\n",
    "    #  Wort muss mindestens min_th mal vorgekommen sein, damit es nicht als Noise angesehen wird\n",
    "    if np.sum(v) > min_th:  \n",
    "        ILF_scores.append((np.log(N_documents/np.count_nonzero(v)), k))\n",
    "        \n",
    "# sortiere und filtere nach den ortsbezogensten Wörtern\n",
    "ILF_scores = sorted(ILF_scores, reverse=True)\n",
    "max_local_words = 20\n",
    "for elt in ILF_scores[:max_local_words]:\n",
    "    print(elt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Language Ensemble </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. FastText Skripte</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die FastText Skripte werden auf der Kommandozeile ausgeführt. Prinzipiell kann auch das Python-Modul fastText mit identischer Funktionalität importiert werden. Hierdurch müssen jedoch Performanceeinbußen in Kauf genommen werden. Die nachfolgende Anleitung zeigt, wie ein beispielhafter Ablauf stattdessen auf der Kommandozeile durchgeführt werden kann. Es wird im Folgenden vorausgesetzt, dass fastText bereits installiert ist. Sämtliche nachfolgenden Schritte können im Ordner *text_embeddings* durchgeführt werden.\n",
    "\n",
    "*cd text_embeddings/*\n",
    "\n",
    "**Trainingspart**\n",
    "\n",
    "Die Umwandlung von Texten erfolgt in zwei Schritten. Zunächst wird der Tweet-Text in eine Satzvektorrepräsentation überführt und diese im nächsten Schritt vom neuronalen Netz des fastText Modells vorhergesagt. Die Satzvektorrepräsentation muss entweder in einem unüberwachten Trainingsschritt gelernt werden. Alternativ kann eine vortrainierte Version bereits heruntergeladen werden. Wie im Bericht beschrieben, arbeiten wir mit vortrainierten Vektoren verschiedener sprachen, welche sich in einem ausgerichteten Vektorraum befinden.\n",
    "\n",
    "In der Datei *emb_locations.txt* sind die URLs von vortrainierten, ausgerichteten Vektorfiles enthalten. Mit Hilfe des Skriptes *crawl_pretrained_embeddings.sh* können diese heruntergeladen werden (Vorsicht: Größe aller Daten zusammen etwa 24 GB).\n",
    "\n",
    "*chmod +x crawl_pretrained_embeddings.sh*\n",
    "\n",
    "*./crawl_pretrained_embeddings.sh*\n",
    "\n",
    "Wir arbeiten mit einem kombinierten, d.h. sprachunabhängigen Vektorfile. Daher müssen alle Teilsprachen zusammengefügt werden:\n",
    "\n",
    "*cat *.vec > multi.vec*\n",
    "\n",
    "Das neu enstandende *multi.vec* File enthält alle Embedding-Repräsentationen von Wörtern aller gewünschter Sprachen. Mit diesem kann nun der zweite Schritt durchgeführt werden, d.h. in einem überwachten Lernprozess können Wortvektoren auf eines der 150 Labels abgebildet werden. Der Übersichtlichkeit halber wird bereits ein Trainingsfile von Tweettexten mit zugehörigen Labels bereit gestellt, siehe die Dateien *train.texts* bzw. *train.labels*.\n",
    "\n",
    "*head tweets.texts*\n",
    "\n",
    "*head tweets.labels*\n",
    "\n",
    "Für den supervised Part erwartet FastText jedoch ein einziges Inputfile, welches Labels und Text kombiniert in folgender Form bereit stellt:\n",
    "\n",
    "\\_\\_label\\_\\_*label*$_1$ *text*$_1$ \n",
    "\n",
    "\\_\\_label\\_\\_*label*$_2$ *text*$_2$ ...\n",
    "\n",
    "Wir führen also eine kurze Konvertierung durch:\n",
    "\n",
    "*paste -d ' ' tweets.labels tweets.texts > tweets.combined*\n",
    "\n",
    "*sed -i -e 's/^/\\_\\_label\\_\\_/' tweets.combined*\n",
    "\n",
    "\n",
    "\n",
    "Bei Betrachtung sieht das entstandene Files gut aus:\n",
    "\n",
    "*head tweets.combined*\n",
    "\n",
    "Ein sauberer Trainings- und Evaluationsprozess benötigt eigentlich eine saubere Trennung von Trainings-, Test- und Validierungsdaten. Das Skript *train_test_split.sh* hat eine solche Trennung für eine 10-fache CV bereits vorimplementiert. Prinzipiell wird der anfängliche Datensatz aus *train.combined* in zehn Teile zertrennt und diese im Verhältnis von 8:1:1 auf Trainings-, Test- und Validierungsdaten für eben 10 Folds aufgeteilt. Für eine möglichst faire Verteilung bietet sich zuvor noch ein Shuffling des Datensatzes an.\n",
    "\n",
    "*shuf tweets.combined -o tweets.combined*\n",
    "\n",
    "*chmod +x train_test_split.sh*\n",
    "\n",
    "*./train_test_split.sh*\n",
    "\n",
    "Es wird hierdurch eine Menge an temporären Files im aktuellen Ordner erstellt. Die *tweets-split.**-Dateien beziehen sich auf die 10 Splits, *tweets.trainN*, *tweets.testN* und *tweets.validN* auf die drei Sets für den Split Nummer $N$.\n",
    "\n",
    "Das eigentliche (supervised) Training findet im nächsten Schritt statt. Theoretisch müsste nur ein Modell trainiert werden, will man jedoch gleich die Modellgüte per Kreuzvalidierung prüfen, so müssen natürlich alle zehn Splits betrachtet werden. Das Skript *train.sh* fasst den Trainingsprozess in einem File zusammen.\n",
    "\n",
    "*chmod +x train.sh*\n",
    "\n",
    "*./train.sh*\n",
    "\n",
    "Hier soll noch kurz auf die Parameter des Trainings eingegangen werden. Betrachten wir beispielsweise die erste Zeile des *train.sh* Skripts:\n",
    "\n",
    "*head -n1 train.sh* liefert\n",
    "\n",
    "fasttext supervised -input tweets.train1 -output tweets.model1 -pretrainedVectors multi.vec -dim 300 -autotune-validation tweets.valid1 -autotune-duration 86400 -thread 40\n",
    "\n",
    "Beim Aufruf des fasttext-Tools kann mit dem Parameter *supervised* der überwachte Trainingsprozess angestoßen werden. Dieser erwartet in jedem Fall (via **-input**) ein Trainings- und (via **-output**) ein Outputfile. \n",
    "\n",
    "Da wir bereits mit fertigen Wortvektoren arbeiten, müssen diese auch übergeben werden via **-pretrainedVectors**. Ohne Parametertuning arbeitet FastText mit genau $100$ Dimensionen für die Embeddings. Die vortrainierten Embeddings bestehen jedoch aus 300 Dimensionen, weswegen auch dies als Parameter angegeben werden muss: **-dim 300**. \n",
    "\n",
    "Schlussendlich kann vor dem Training ein Hyperparametertuning mit einem optionalen Validierungsset durchgeführt werden. Hierzu muss das entsprechende File übergeben werden **-autotune-validation**, sowie eine Zeit in Sekunden, welche für das Tuning verwendet werden darf, hier bspw. **-autotune-duration**.\n",
    "\n",
    "Output des überwachten Trainings sind einerseits neue **.vec* Vektordateien (werden nicht benötigt, da wir ja bereits unser eigenes File besitzen), sowie die **fertigen *.bin Modelle**. Diese brauchen wir natürlich schon für die Vorhersage.\n",
    "\n",
    "Optional können die trainierten Modelle an dieser Stelle noch evaluiert werden. Fasttext bietet dazu das Kommandozeilenargument **fasttext test [model] [test-data][k][th]**. **model** bezieht sich auf eines der soeben erhaltenen **.bin*-Modelle, **test-data** auf eines der *tweets.testN*-Files, $k$ und $th$ auf optionale Optimierungen mit der Vorhersage der $k$ besten Labels, welche jeweils einen minimalen Wahrscheinelichkeits-Grenzwert von $th$ erreichen müssen. Das Skript *test.sh* führt für alle zehn Folds eine Reihe von Tests mit variierenden Werten für $k\\in[1;3]$ und $th=0.15\\cdot i$ mit $i\\in[0;4]$ durch.\n",
    "\n",
    "*chmod +x test.sh*\n",
    "\n",
    "*./test.sh*\n",
    "\n",
    "Die Ergebnisse können in Textform in den neu entstehenden *tweets.evalN* Dateien ausgelesen werden.\n",
    "\n",
    "**Anwendungspart**\n",
    "\n",
    "Für die Konvertierung und Vorhersage von Texten aus Tweets wird das soeben trainierte *tweets.modelN.bin* File benötigt.\n",
    "\n",
    "Ein derartiges Skript ist auch für diesen Task in der Datei *predict_texts.sh* vorimplementiert. Der Aufbau ist hier ähnlich wie zum bereits ausgeführten *0-filter_on_geotagged_tweets.sh*-Preprocessingskript.\n",
    "\n",
    "Zunächst werden sämtliche *.texts*-Dateien aus dem am Anfang dieses Notebooks angegebenen *feature_files*-Ordners aufgelistet und jeder Dateipfad an die Funktion *processfile()* übergeben. In dieser wird jede Zeile des Dokuments mit dem Modell vorhergesagt, wobei als optimierte Hyperparameter $k=10$ und $th=0.05$ gesetzt sind. Es kann sinnvoll sein, dass mehr Labels mit niedrigeren Grenzwerten für $th$ zurückgeliefert werden. Hier können die Parameter nach eigenem Ermessen variiert werden. Die Ausgabe der probabilistischen Vorhersage wird zeilenweise in ein gleichnamiges **.pred* File geschrieben.\n",
    "\n",
    "*head -n3 *.pred* liefert beispielsweise\n",
    "\n",
    "\\_\\_label\\_\\_84 0.951783\n",
    "\n",
    "\\_\\_label\\_\\_56 0.741892 \\_\\_label\\_\\_28 0.133467 \\_\\_label\\_\\_84 0.100436\n",
    "\n",
    "\\_\\_label\\_\\_56 0.630091 \\_\\_label\\_\\_23 0.155819 \\_\\_label\\_\\_42 0.140562\n",
    "\n",
    "Diese Ergebnisse sind also bereits die Vorhersageergebnisse des Algorithmus, aber müssen noch etwas aufbereitet werden, bspw. in eine Matrixdarstellung. Wir verwenden dazu wieder eine ähnliche dreistufige Pipeline wie bereits bekannt:\n",
    "1. zunächst werden sämtliche **.pred* Files aufgelistet und diese als Work-Batches auf verschiedene Arbeiterprozesse aufgeteilt\n",
    "2. Jeder Arbeiterprozess verarbeitet ein File, d.h. einige Tausend Elemente. Er tut dies, indem er das File zeilenweise ausliest, d.h. jeweils einen Tweet individuell betrachtet, und diesen \n",
    "3. gemäß einer festen Verarbeitungsvorschrift konvertiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Konvertierung eines Tweets (bzw. des Vorhersageergebnisses eines Tweets) in Vektorrepräsentation\n",
    "\n",
    "def embedding_prediction_to_distribution(line):\n",
    "    vec = np.zeros(N_classes + 1)  # probability distribution\n",
    "\n",
    "    splits = line.split(\"__label__\")\n",
    "\n",
    "    if len(splits) == 1:  # no prediction possible\n",
    "        vec[-1] = 1\n",
    "    else:\n",
    "        for split in splits[1:]:\n",
    "            probs = split.split(\" \")\n",
    "            vec[int(probs[0])-1] = float(probs[1])\n",
    "        vec[-1] = 1 - np.sum(vec)\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Konvertierung eines Text-Vorhersagefiles in eine Matrixrepräsentation (+Persistierung)\n",
    "def run(file_list, show_progress=False):\n",
    "    parts = []\n",
    "    \n",
    "    if show_progress:\n",
    "        for file in tqdm(file_list):\n",
    "            X = np.empty((0, N_classes + 1))\n",
    "            X = csr_matrix(X)\n",
    "\n",
    "            n_rows = 0\n",
    "            with open(file, \"r\") as f:\n",
    "                has_next = True\n",
    "\n",
    "                while has_next:\n",
    "\n",
    "                    try:\n",
    "                        line = next(f).strip()\n",
    "                        parts.append(embedding_prediction_to_distribution(line))\n",
    "                        n_rows += 1\n",
    "                    except StopIteration:\n",
    "                        has_next = False\n",
    "                        continue\n",
    "\n",
    "            X = vstack((X, parts))\n",
    "            save_npz(file + \".npz\", X)\n",
    "            if X.shape[0] != n_rows:\n",
    "                print(X.shape, n_rows, file)\n",
    "            parts = []\n",
    "    else:\n",
    "        for file in file_list:\n",
    "            X = np.empty((0, N_classes + 1))\n",
    "            X = csr_matrix(X)\n",
    "            n_rows = 0\n",
    "\n",
    "            with open(file, \"r\") as f:\n",
    "                has_next = True\n",
    "\n",
    "                while has_next:\n",
    "                    try:\n",
    "                        n_rows += 1\n",
    "                        line = next(f).strip()\n",
    "                        parts.append(embedding_prediction_to_distribution(line))\n",
    "\n",
    "                    except StopIteration:\n",
    "                        has_next = False\n",
    "                        continue\n",
    "\n",
    "            X = vstack((X, parts))\n",
    "            save_npz(file + \".npz\", X)\n",
    "            print(X.shape, n_rows)\n",
    "            parts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 43.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Identifikation von Vorhersage-Files und Zuordnung der Workbatches\n",
    "N_cores = 1\n",
    "path = \"data/feature_files/\"\n",
    "N_classes = 150\n",
    "\n",
    "files = [path + f for f in listdir(path) if f[-4:] == \"pred\" and isfile(join(path, f))]\n",
    "\n",
    "batch_size = math.ceil(len(files) / N_cores)\n",
    "\n",
    "workers = [Process(target=run, args=(files[i*batch_size:(i+1)*batch_size], i == 0)) for i in range(N_cores)]\n",
    "\n",
    "for w in workers:\n",
    "    w.start()\n",
    "    \n",
    "for w in workers:\n",
    "    w.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kann das Ergebnis der Text-Vorhersage eines einzelnen Tweets eingesehen werden. Die Ergebnismatrix für das komplette File ist als Sparse Matrix implementiert, um Speicherplatz zu sparen. Um die Matrix einzusehen, empfiehlt es sich daher zunächst, mit der Funktion *todense()* die Konvertierung in eine herkömmliche (dicht besetzte) Matrix durchzuführen.\n",
    "\n",
    "Wie unten ersichtlich, ist der Output ein $151$-dimensionaler Vektor. Die ersten $150$ Dimensionen korrespondieren zu den Wahrscheinlichkeiten, dass der Text dem Modell zufolge einer der $150$ Zielklassen entstammt, **falls** der Wahrscheinlichkeitswert über $0.05$ liegt (sonst ist er auf $0$ gesetzt). Dei letzte, d.h. $151.$ Dimension gibt die Gegenwahrscheinlichkeit, also den Rauschfaktor des Ergebnisses an. Dieser Wert ist letztlich die Summe der Wahrscheinlichkeiten aller Zielklassen $< 0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.140562,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.630091, 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.155819,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "         0.073528]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = load_npz(path + \"tweets-2020-02-01_00:03:35.xz.texts.pred.npz\")\n",
    "X.todense()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
