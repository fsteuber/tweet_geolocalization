{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f40803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1724885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mapping\n",
    "C = pickle.load(open(\"layer_mappings.pcl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "003a0992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 4, 8, 16])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401f054",
   "metadata": {},
   "source": [
    "**Evaluations on small dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba681a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# create confusion matrix\n",
    "path = \"/mnt/nas/mnt/erebor1/tweetlocator_snam/\"\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with open(path + \"en.test1\") as ground_truth_file:\n",
    "    with open(path + \"en.test1.predict\") as predictions_file:\n",
    "        i = 0\n",
    "        limit = 10**8  # restrict to N datapoints\n",
    "        while True:\n",
    "            if i >= limit:\n",
    "                break\n",
    "            i += 1\n",
    "            true = ground_truth_file.readline()\n",
    "            pred = predictions_file.readline()\n",
    "            \n",
    "            if len(true) == 0:  # eof\n",
    "                break\n",
    "                \n",
    "            true = true.split(\" \")[0] # split label and data\n",
    "            y_true.append(int(true.split(\"__\")[-1]))  # grab ground truth label\n",
    "            y_pred.append(int(pred.split(\"__\")[-1])) # grab predicted label\n",
    "            \n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49c67a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7074856970510324\n",
      "Recall: 0.7074856970510324\n",
      "F1-Score: 0.7074856970510324\n"
     ]
    }
   ],
   "source": [
    "# model qualities on highest layer (l1)\n",
    "\n",
    "# create mapping (four classes)\n",
    "mapping_l1 = {}\n",
    "for layer_id in range(len(C[4])):\n",
    "    for polygon_id in C[4][layer_id]:\n",
    "        mapping_l1[polygon_id] = layer_id\n",
    "\n",
    "# convert true/predicted labels with layer mapping\n",
    "y_true_l1 = [mapping_l1[x] for x in y_true]\n",
    "y_pred_l1 = [mapping_l1[x] for x in y_pred]\n",
    "\n",
    "print(\"Precision: {}\".format(precision_score(y_true_l1, y_pred_l1, average=\"micro\")))\n",
    "print(\"Recall: {}\".format(recall_score(y_true_l1, y_pred_l1, average=\"micro\")))\n",
    "print(\"F1-Score: {}\".format(f1_score(y_true_l1, y_pred_l1, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00403962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6051513217319876\n",
      "Recall: 0.6051513217319876\n",
      "F1-Score: 0.6051513217319876\n"
     ]
    }
   ],
   "source": [
    "# model qualities on intermediate layer (l2)\n",
    "\n",
    "# create mapping (16 classes)\n",
    "mapping_l2 = {}\n",
    "for layer_id in range(len(C[16])):\n",
    "    for polygon_id in C[16][layer_id]:\n",
    "        mapping_l2[polygon_id] = layer_id\n",
    "\n",
    "# convert true/predicted labels with layer mapping\n",
    "y_true_l2= [mapping_l2[x] for x in y_true]\n",
    "y_pred_l2= [mapping_l2[x] for x in y_pred]\n",
    "\n",
    "print(\"Precision: {}\".format(precision_score(y_true_l2, y_pred_l2, average=\"micro\")))\n",
    "print(\"Recall: {}\".format(recall_score(y_true_l2, y_pred_l2, average=\"micro\")))\n",
    "print(\"F1-Score: {}\".format(f1_score(y_true_l2, y_pred_l2, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8bcfa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5974065700433394\n",
      "Recall: 0.5974065700433394\n",
      "F1-Score: 0.5974065700433394\n"
     ]
    }
   ],
   "source": [
    "# model qualities on lowest layer (l3)\n",
    "print(\"Precision: {}\".format(precision_score(y_true, y_pred, average=\"micro\")))\n",
    "print(\"Recall: {}\".format(recall_score(y_true, y_pred, average=\"micro\")))\n",
    "print(\"F1-Score: {}\".format(f1_score(y_true, y_pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d78d57",
   "metadata": {},
   "source": [
    "**Evaluations on large dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "365b8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# create confusion matrix\n",
    "path = \"/mnt/nas/mnt/erebor1/tweetlocator_snam/\"\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with open(path + \"en.test2\") as ground_truth_file:\n",
    "    with open(path + \"en.test2.predict\") as predictions_file:\n",
    "        i = 0\n",
    "        limit = 10**9  # restrict to N datapoints\n",
    "        while True:\n",
    "            if i >= limit:\n",
    "                break\n",
    "            i += 1\n",
    "            true = ground_truth_file.readline()\n",
    "            pred = predictions_file.readline()\n",
    "            \n",
    "            if len(true) == 0:  # eof\n",
    "                break\n",
    "                \n",
    "            true = true.split(\" \")[0] # split label and data\n",
    "            y_true.append(int(true.split(\"__\")[-1]))  # grab ground truth label\n",
    "            y_pred.append(int(pred.split(\"__\")[-1])) # grab predicted label\n",
    "            \n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4612fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7135346282670418\n",
      "Recall: 0.7135346282670418\n",
      "F1-Score: 0.713534628267042\n"
     ]
    }
   ],
   "source": [
    "# model qualities on highest layer (l1)\n",
    "\n",
    "# create mapping (four classes)\n",
    "mapping_l1 = {}\n",
    "for layer_id in range(len(C[4])):\n",
    "    for polygon_id in C[4][layer_id]:\n",
    "        mapping_l1[polygon_id] = layer_id\n",
    "\n",
    "# convert true/predicted labels with layer mapping\n",
    "y_true_l1 = [mapping_l1[x] for x in y_true]\n",
    "y_pred_l1 = [mapping_l1[x] for x in y_pred]\n",
    "\n",
    "print(\"Precision: {}\".format(precision_score(y_true_l1, y_pred_l1, average=\"micro\")))\n",
    "print(\"Recall: {}\".format(recall_score(y_true_l1, y_pred_l1, average=\"micro\")))\n",
    "print(\"F1-Score: {}\".format(f1_score(y_true_l1, y_pred_l1, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "529cc08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6169727602183437\n",
      "Recall: 0.6169727602183437\n",
      "F1-Score: 0.6169727602183437\n"
     ]
    }
   ],
   "source": [
    "# model qualities on intermediate layer (l2)\n",
    "\n",
    "# create mapping (16 classes)\n",
    "mapping_l2 = {}\n",
    "for layer_id in range(len(C[16])):\n",
    "    for polygon_id in C[16][layer_id]:\n",
    "        mapping_l2[polygon_id] = layer_id\n",
    "\n",
    "# convert true/predicted labels with layer mapping\n",
    "y_true_l2= [mapping_l2[x] for x in y_true]\n",
    "y_pred_l2= [mapping_l2[x] for x in y_pred]\n",
    "\n",
    "print(\"Precision: {}\".format(precision_score(y_true_l2, y_pred_l2, average=\"micro\")))\n",
    "print(\"Recall: {}\".format(recall_score(y_true_l2, y_pred_l2, average=\"micro\")))\n",
    "print(\"F1-Score: {}\".format(f1_score(y_true_l2, y_pred_l2, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac96a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6096328078016952\n",
      "Recall: 0.6096328078016952\n",
      "F1-Score: 0.6096328078016952\n"
     ]
    }
   ],
   "source": [
    "# model qualities on lowest layer (l3)\n",
    "print(\"Precision: {}\".format(precision_score(y_true, y_pred, average=\"micro\")))\n",
    "print(\"Recall: {}\".format(recall_score(y_true, y_pred, average=\"micro\")))\n",
    "print(\"F1-Score: {}\".format(f1_score(y_true, y_pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b5293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
